# 斯坦福课程「CS146S现代软件开发者」W1D5：ollama搭建 & 用python脚本和LLM对话

在本节课中，我们将完成 **Ollama** 的环境搭建，通过“磨刀不误砍柴工”的方式快速掌握必要的 Python 基础知识，并开启你的第一次 LLM 自动化对话。

---

## 1. 安装 Ollama

Ollama 是一个能够在本地轻松运行大型语言模型的工具。它在我们的开发流程中扮演着“模型服务器”的角色。

### macOS 安装
如果你使用 Homebrew，可以运行：
```bash
brew install --cask ollama 
ollama serve
```

### Linux 安装
运行以下命令：
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows 安装
从 [ollama.com/download](https://ollama.com/download) 下载并运行安装程序。

### 验证安装
在终端运行以下命令，如果显示版本号即表示安装成功：
```bash
ollama -v
```

---

## 2. 准备模型与常用操作

### 下载模型
我们要准备两个“大脑”，一个轻量级，一个功能全面：
1.  **ministral-3:3b**: 一个轻量级但功能强大的本地模型。
2.  **gemini-3-flash-preview:cloud**: 通过 Ollama 调用云端的 Gemini 模型（限量免费，无需配置 API Key）。

运行以下命令：
```bash
ollama pull ministral-3:3b
ollama pull gemini-3-flash-preview:cloud
```

### 角色划分：数据是如何流转的？
在开始写代码前，理解这三个层次以及它们如何“说话”至关重要：
1.  **Ollama App (后台服务)**: 负责硬件调度、模型加载。它实际上运行在你的本地端口（默认 11434）。
2.  **Ollama SDK (Python 库)**: 它是桥梁。你在代码里写的是 Python 的列表和字典，SDK 会把它们转换成 **JSON 格式**（一种通用的文本数据交换格式）并发送给 App。
3.  **LLM 模型 (如 Ministral)**: 对话的“大脑”。

> **知识点：什么是“发送 JSON”？**
> 当你调用 `chat()` 时，SDK 底层会将你的请求打包成类似这样的文本：
> `{"model": "ministral", "messages": [{"role": "user", "content": "你好"}]}`。
> 这种格式跨语言通用，是现代软件开发中程序与程序之间对话的“普通话”。

### 终端操作速查
你可以直接在终端和模型聊天或管理它们：
- `ollama list`: 查看本地模型。
- `ollama run <模型名>`: 开始即时对话。
- `ollama ps`: 查看当前哪些模型在占用你的内存。

---

## 3. Python 基础预备课：什么是模块与函数？

在正式写代码之前，我们需要搞清楚代码里的这两个核心角色。对于 AI 开发来说，它们就是你的“工具箱”和“加工厂”。

### 3.1 模块 (Module)：你的“魔法工具箱”
Python 代码通常非常简洁，是因为我们可以通过 `import`（导入）直接搬运别人写好的专业工具。
- **通俗比喻**：就像你组装家具，不需要自己从砍树开始，而是直接去宜家买回一个个零件包（模块）。
- **代码体现**：
  ```python
  import os          # 导入名为 os 的工具箱，用于操作文件和目录
  from ollama import chat  # 从 ollama 工具箱中，只取出“聊天”这个特定工具
  ```

### 3.2 进阶概念：模块 (Module) vs SDK
很多同学会听到这两个词，它们的差别在于：
- **模块 (Module)**：是一个**物理单位**。它是 Python 代码的存放形式。你在代码里 `import` 的那个名字，就是一个模块或包。
- **SDK (Software Development Kit)**：是一个**逻辑集合**。它是“软件开发工具包”的缩写。一个 SDK 往往包含了一个或多个模块，还包括了配套的文档和示例，专门为了让开发者更方便地使用某个特定平台（比如 Ollama）。
- **通俗比喻**：
  - **模块**就像是那把具体的“螺丝刀”。
  - **SDK**就像是“某品牌电器的专业维修工具包”，里面既有螺丝刀（模块），也有说明书（文档），是一整套解决方案。

### 3.3 函数 (Function)：你的“自动化加工厂”
函数是一段可以重复使用的、带有特定功能的代码块。
- **通俗比喻**：函数就像一台果汁机。你投入水果（**输入**），它执行榨汁动作（**流程**），最后产出果汁（**输出**）。
- **代码体现**：
  ```python
  def make_juice(fruit):  # 定义一个名为 make_juice 的加工厂，需要水果作为输入
      print(f"正在加工 {fruit}...")
      return f"{fruit} 汁" # 产出结果
  ```

---

## 4. 你的第一次对话：`intro_chat.py`

现在，我们将把“工具箱”（模块）和“加工厂”（函数）结合起来，实现第一次 AI 对话。

#### 4.1 源码分块讲解
**源码文件:** `week1/intro_chat.py`

**第一块：准备工具箱（模块与 SDK）**
```python
import os  # 导入系统库
from dotenv import load_dotenv  # 导入环境变量管理工具
from ollama import chat  # 导入 Ollama SDK 的对话函数
```
**详解:**
我们要把操作系统的功能、环境配置功能以及 AI 对话功能全部“搬进”我们的程序。

**第二块：定义对话加工厂**
```python
load_dotenv() # 启动配置加载器

def first_chat_demo(): # 定义一个名为“第一次对话演示”的函数
    response = chat(
        model="ministral-3:3b",
        messages=[
            {"role": "system", "content": "你是一个友好的助教。"},
            {"role": "user", "content": "你好！什么是提示词工程？"},
        ],
        options={"temperature": 0.7}
    )
    
    # print(response) # 如果你好奇 response 的完整内容，可以取消这行的注释
    print(response.message.content)
```
**详解:**
- **`chat` 是一个预设好的加工厂（函数）**：
    - 它的定义已经由 Ollama SDK 的开发者写好了。
    - 它负责把我们输入的“配料”传给 LLM 大脑，并把大脑的回答“加工”成一个结构化的结果送回来。
- **参数赋值**：就像 `make_juice(fruit="苹果")`。在 `chat(...)` 的圆括号里，我们通过 `model=...`, `messages=...` 给这些“插槽”赋了值。
- **`response` 是产出的“成品” (变量)**：
    - 注意：`response` 不是函数，它是 `chat` 加工厂运行完之后 **return（返回）** 给我们的结果。
    - 就像果汁机运行完后给你的一杯“果汁”。
- **温度 (Temperature)：AI 的性格调节阀**
    - 它其实是一个数学参数，控制模型生成下一个字时的“随机程度”。
    - **0.0 (绝对理性)**：模型会选择概率最高的那个词。适合写代码、解数学题、抓取信息。
    - **0.7 (温和稳重)**：最常用的数值。回答有一定变化但逻辑清晰。适合日常助教、翻译。
    - **1.0+ (放飞自我)**：模型开始尝试低概率的词，脑洞大开。适合脑暴、写诗、给剧本起名。

**第三块：启动程序**
```python
if __name__ == "__main__":
    first_chat_demo()
```
**详解 (程序的“启动开关”)**：
由于一个项目可能有多个文件，这行代码的作用是：告诉 Python，“如果这个文件是被直接运行的，请立刻执行 `first_chat_demo()` 加工厂”。

#### 4.4 避坑指南：大模型的“金鱼记忆” (Stateless)
很多同学连续运行两次脚本，会奇怪：“为什么模型不记得我上一分钟说过的话？”
- **真相**：原生的 LLM 接口是**无状态的 (Stateless)**。它像鱼一样只有 7 秒记忆，每一次 `chat()` 对它来说都是全新的。
- **如何让它记得？**：在下周的课程中，我们会学习如何手动把之前的对话历史塞进 `messages` 列表里再传给它。

---

## 5. 现代开发者的调试理念

在准备 W1D5 的过程中，你可能会遇到报错（比如 `ConnectError`）。

### 5.1 善用 AI IDE 的 Agent
如果你在运行 `intro_chat.py` 时报错，可以选中代码并询问你的 AI IDE（如 Google Antigravity）。它是一个高阶 **Agent**，能够理解你的上下文并直接给出解决方案。

### 5.2 练习手感 vs 一步到位
虽然 Antigravity 能够一步到位帮你就地改好代码，但在学习初期，我们**鼓励你先通过 Chat 模式询问思路，或者口头要求 Antigravity 给你提供思路，然后自己手动排查问题**。
- **为什么要手动？**：就像学开车不能全靠自动驾驶。只有当你亲手理解了“服务未启动”和“代码写错”的区别，你才能在大考或复杂工程中建立扎实的基础。

---

## 6. 课例总结与练习

### 总结
1.  **角色**：App 是引擎，SDK 是方向盘，Model 是路线。
2.  **开发流程**：导入模块 -> 定义函数 -> 调用 SDK -> 处理输出。
3.  **调试理念**：
    - **手动**：理解错误原因，建立扎实基础。
    - **一步到位**：快速修复，适合实际开发。

### 练习
1.  运行 `intro_chat.py`。
2.  **探索全貌**：取消 `print(response)` 前面的 `#` 号，再次运行。观察控制台输出的那个长长的内容，找找看 `message.content` 藏在哪里？       
3.  **温度实验室**：
    - 将 `temperature` 改为 `0.0`，运行三次。你会发现模型对同一个问题的回答几乎一模一样。
    - 将 `temperature` 改为 `1.5`，看看模型是否开始变得“胡言乱语”或极具创意？
4.  修改 `system` 消息，尝试让模型扮演武侠小说里的人物。
5.  尝试删除 `print(response.message.content)` 这行，看看发生了什么？你会发现即使 AI 回答了，你却看不到输出（这证明了加工厂产出了东西，但你没把它摆在桌面上）。
